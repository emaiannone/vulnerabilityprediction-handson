{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fundamentals of Mining Software Repositories for Vulnerability Prediction - The Practical Perspective\n",
    "\n",
    "Preliminary steps:\n",
    "- Open a console tab and run this command: `!pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import math\n",
    "import signal\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from random import random\n",
    "from re import match\n",
    "from os import listdir, makedirs\n",
    "from os.path import *\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, f1_score, precision_score, recall_score)\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_api(url, try_num=1, max_try=None):\n",
    "    try:\n",
    "        headers = {'Accept': 'application/json'}\n",
    "        return requests.get(url, headers=headers).json()\n",
    "    except:\n",
    "        time.sleep(2**try_num + random() * 0.01)\n",
    "        if max_try and try_num < max_try:\n",
    "            return call_api(url, try_num=try_num+1)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def store_chunk(items, filepath, num_chunk, lower, upper=None):\n",
    "    chunk = {k: v for (k, v) in items[lower:upper]}\n",
    "    num_chunk_str = \"0\" + str(num_chunk) if num_chunk < 10 else str(num_chunk)\n",
    "    dest_file = join(dirname(filepath), num_chunk_str + \"_\" + basename(filepath))\n",
    "    with open(dest_file, \"w\") as json_file:\n",
    "        json.dump(chunk, json_file, indent=2)\n",
    "\n",
    "def store_cves(data, filepath, chunk_size=1000, rewrite_past_chunks=False):\n",
    "    items = [(k, v) for k, v in data.items()]\n",
    "    selected = 0\n",
    "    num_chunk = 0\n",
    "    while len(items) - selected > chunk_size:\n",
    "        upper = selected + chunk_size\n",
    "        if rewrite_past_chunks:\n",
    "            store_chunk(items, filepath, num_chunk, selected, upper)\n",
    "        selected = upper\n",
    "        num_chunk += 1\n",
    "    if len(items) - selected > 0:\n",
    "        store_chunk(items, filepath, num_chunk, selected)\n",
    "        \n",
    "def read_cves(filepath):\n",
    "    dn = dirname(filepath)\n",
    "    bn = basename(filepath)\n",
    "    files = [f for f in listdir(dn) if match(r\"[0-9]*_\" + bn, f)]\n",
    "    files.sort()\n",
    "    data = {}\n",
    "    for f in files:\n",
    "        path = join(dn, f)\n",
    "        if exists(path) and getsize(path):\n",
    "            with open(path, \"r\") as in_file:\n",
    "                data.update(json.load(in_file))\n",
    "    return data\n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    global stop_signal\n",
    "    print('Going to gracefully stop')\n",
    "    stop_signal = True\n",
    "    \n",
    "def str_to_datetime(date_str, formats=[\"%Y-%m-%dT%H:%M:%S\"]):\n",
    "    if not date_str:\n",
    "        return None\n",
    "    for f in formats:\n",
    "        try:\n",
    "            converted = dt.datetime.strptime(date_str, f)\n",
    "            return converted\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def datetime_to_str(date: dt.datetime, formats=[\"%Y-%m-%dT%H:%M:%S\"]):\n",
    "    for f in formats:\n",
    "        try:\n",
    "            converted = date.strftime(f)\n",
    "            return converted\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "def get_words(tokens):\n",
    "    return [t for t in tokens if len(t) > 2]\n",
    "\n",
    "def get_eng_words(wordz, eng_dict):\n",
    "    return [w for w in wordz if w in eng_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "The following cell extracts/collects the CVEs in CVE Search dump downloaded from https://www.cve-search.org/dataset/ and cominbing them with CVE Search API data at https://cve.circl.lu/api/.\n",
    "\n",
    "For the sake of simplicity, the dump has been reduced to its 5% content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7415/7415 [42:12<00:00,  2.93it/s]\n"
     ]
    }
   ],
   "source": [
    "endpoint = \"https://cve.circl.lu/api/cve\"\n",
    "\n",
    "dump_content = {}\n",
    "with open(\"../data/dump/partial_dump.json\", \"r\") as in_file:\n",
    "    dump_content = json.loads(in_file.read())\n",
    "raw_cves = dump_content[\"cves\"]\n",
    "\n",
    "collected_cves_file = \"../data/collected/collected_cves.json\"\n",
    "collected_cves = read_cves(collected_cves_file)\n",
    "discarded_cves_file = \"../data/discarded/discarded_cves.json\"\n",
    "discarded_cves = read_cves(discarded_cves_file)\n",
    "\n",
    "store_once_every = 100\n",
    "stop_signal = False\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "for idx, cve in enumerate(tqdm(raw_cves)):\n",
    "    if stop_signal:\n",
    "        break\n",
    "    if cve in collected_cves:\n",
    "        continue\n",
    "    try:\n",
    "        if not match(r\"CVE-\\d{4}-\\d{4,7}\", cve):\n",
    "            discarded_cves[cve] = {\"reason\": \"MALFORMED_CVE\", \"detail\": cve}\n",
    "            store_cves(discarded_cves, discarded_cves_file)\n",
    "            continue\n",
    "        try:\n",
    "            response = call_api(join(endpoint, cve), max_try=5)\n",
    "        except Exception as e:\n",
    "            discarded_cves[cve] = {\"reason\": \"API_ERROR\", \"detail\": str(e)}\n",
    "            store_cves(discarded_cves, discarded_cves_file)\n",
    "            continue\n",
    "            \n",
    "        # Select any other data from dump or API, run other mining algorithms, etc.\n",
    "        collected_cves[cve] = {\n",
    "            \"date\": response[\"Published\"] if response and \"Published\" in response else None,\n",
    "            \"description\": response[\"summary\"] if response and \"summary\" in response else None,\n",
    "            \"cwe\": response[\"cwe\"] if response and \"cwe\" in response else None,\n",
    "            \"cvss\": response[\"cvss\"] if response and \"cvss\" in response else None,\n",
    "            \"cvss-vector\": response[\"cvss-vector\"] if response and \"cvss-vector\" in response else None,\n",
    "        }\n",
    "        # Store the current results into storage\n",
    "        if idx + 1 == len(raw_cves) or len(collected_cves) % store_once_every == 0:\n",
    "            store_cves(collected_cves, collected_cves_file)\n",
    "    except Exception as e:\n",
    "        discarded_cves[cve] = {\"reason\": \"UNEXPECTED\", \"detail\": str(e)}\n",
    "        store_cves(discarded_cves, discarded_cves_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "The following cell applies additional cleaning and filtering actions to make the dataset exportable for other tasks, not necessarily ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14430/14430 [00:31<00:00, 454.75it/s]\n"
     ]
    }
   ],
   "source": [
    "collected_cves_file = \"../data/collected/collected_cves.json\"\n",
    "collected_cves = read_cves(collected_cves_file)\n",
    "discarded_cves_file = \"../data/discarded/discarded_cves.json\"\n",
    "discarded_cves = read_cves(discarded_cves_file)\n",
    "prepared_cves_file = \"../data/prepared/prepared_cves.json\"\n",
    "prepared_cves = read_cves(prepared_cves_file)\n",
    "\n",
    "store_once_every = 100\n",
    "stop_signal = False\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "for idx, (cve, cve_data) in enumerate(tqdm(collected_cves.items())):\n",
    "    if stop_signal:\n",
    "        break\n",
    "    if cve in prepared_cves:\n",
    "        continue\n",
    "    try:\n",
    "        # Convert dates to yyyy-mm-dd format, CVSS Score to float\n",
    "        date = str_to_datetime(cve_data[\"date\"]) if \"date\" in cve_data else None\n",
    "        descr = cve_data[\"description\"] if \"description\" in cve_data else None\n",
    "        cvss = float(cve_data[\"cvss\"]) if \"cvss\" in cve_data else None\n",
    "        \n",
    "        # Dropping CVEs\n",
    "        ## Out of scope (too old)\n",
    "        if not date or date.year < 2015:\n",
    "            discarded_cves[cve] = {\"reason\": \"TOO_OLD\", \"detail\": str(date)}\n",
    "            store_cves(discarded_cves, discarded_cves_file)\n",
    "            continue\n",
    "        ## Without description (cannot be imputed)\n",
    "        if not descr or len(descr)==0:\n",
    "            discarded_cves[cve] = {\"reason\": \"NO_DESCRIPTION\", \"detail\": cve_data[\"decription\"]}\n",
    "            store_cves(discarded_cves, discarded_cves_file)\n",
    "            continue\n",
    "        ## Without CVSS\n",
    "        if not cvss:\n",
    "            discarded_cves[cve] = {\"reason\": \"NO_CVSS\", \"detail\": str(cvss)}\n",
    "            store_cves(discarded_cves, discarded_cves_file)\n",
    "            continue\n",
    "        \n",
    "        # Initial cleaning of description: convert \\r to \\n in description, remove double spaces\n",
    "        descr = descr.replace(\"\\r\", \"\\n\").replace(\" +\", \" \")\n",
    "        \n",
    "        # Fix CVEs with CVSS out of [0,10] using an heuristic\n",
    "        if cvss < 0:\n",
    "            cvss = 0\n",
    "        if cvss > 10:\n",
    "            cvss = 10\n",
    "        \n",
    "        prepared_cves[cve] = {\n",
    "            \"date\": datetime_to_str(date, formats=[\"%Y-%m-%d\"]),\n",
    "            \"description\": descr,\n",
    "            \"cvss\": cvss,\n",
    "            \"cwe\": response[\"cwe\"]\n",
    "        }\n",
    "\n",
    "        # Store the current results into storage\n",
    "        if idx + 1 == len(collected_cves) or len(prepared_cves) % store_once_every == 0:\n",
    "            store_cves(prepared_cves, prepared_cves_file)\n",
    "    except Exception as e:\n",
    "        discarded_cves[cve] = {\"reason\": \"UNEXPECTED\", \"detail\": str(e)}\n",
    "        store_cves(discarded_cves, discarded_cves_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Setup\n",
    "\n",
    "The following cell prepares both the completed dataset and the training and test splits for a 10 folds stratified cross validation. All the features and labels are determined at this phase, according to the best practices recommended to avoid biases and data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prepared_cves_file = \"../data/prepared/prepared_cves.json\"\n",
    "prepared_cves = read_cves(prepared_cves_file)\n",
    "\n",
    "cves_df = pd.DataFrame.from_dict(prepared_cves, orient='index')\n",
    "cves_df.index.name = \"cve\"\n",
    "\n",
    "# Feature extraction (all data)\n",
    "eng_dict = set(words.words())\n",
    "cves_df[\"TMP_tokens\"] = cves_df[\"description\"].apply(lambda x: tokenize(x))\n",
    "cves_df[\"TMP_words\"]  = cves_df[\"TMP_tokens\"].apply(lambda x: get_words(x))\n",
    "cves_df[\"TMP_eng_words\"]  = cves_df[\"TMP_words\"].apply(lambda x: get_eng_words(x, eng_dict))\n",
    "cves_df[\"nr_tokens\"] = cves_df[\"TMP_tokens\"].apply(lambda x: len(x))\n",
    "cves_df[\"nr_words\"] = cves_df[\"TMP_words\"].apply(lambda x: len(x))\n",
    "cves_df[\"nr_eng_words\"] = cves_df[\"TMP_eng_words\"].apply(lambda x: len(x))\n",
    "cves_df[\"ratio_tokens\"] = cves_df[\"nr_tokens\"] / cves_df[\"nr_tokens\"].sum()\n",
    "cves_df[\"ratio_words\"] = cves_df[\"nr_words\"] / cves_df[\"nr_words\"].sum()\n",
    "cves_df[\"ratio_eng_words\"] = cves_df[\"nr_eng_words\"] / cves_df[\"nr_eng_words\"].sum()\n",
    "del cves_df[\"TMP_tokens\"], cves_df[\"TMP_words\"], cves_df[\"TMP_eng_words\"]\n",
    "\n",
    "# Label assignment (all data)\n",
    "cves_df[\"IS_OUT_OF_BOUND_WRITE\"] = cves_df[\"cwe\"].str.contains(\"cwe-787\", na=False, case=False)\n",
    "\n",
    "# Export the entire dataframe, with all data\n",
    "cves_df.to_csv(\"../data/dataset/dataset.csv\")\n",
    "\n",
    "# 10 random folds\n",
    "splits = StratifiedKFold(n_splits=10, shuffle=True, random_state=42).split(cves_df, cves_df[\"IS_OUT_OF_BOUND_WRITE\"])\n",
    "for idx, (train_index, test_index) in enumerate(splits):\n",
    "    train_df, test_df = cves_df.iloc[train_index].copy(), cves_df.iloc[test_index].copy()\n",
    "    dn = f'../data/dataset/stratified-splits/{idx}'\n",
    "    if not exists(dn):\n",
    "        makedirs(dn)\n",
    "    \n",
    "    # Store precious information seen during training\n",
    "    train_tot_tokens = train_df[\"nr_tokens\"].sum()\n",
    "    train_tot_words = train_df[\"nr_words\"].sum()\n",
    "    train_tot_eng_words = train_df[\"nr_eng_words\"].sum()\n",
    "    \n",
    "    # Training Set: recompute features (training time) and label (testing time), if applicable\n",
    "    train_df[\"ratio_tokens\"] = train_df[\"nr_tokens\"] / train_tot_tokens\n",
    "    train_df[\"ratio_words\"] = train_df[\"nr_words\"] / train_tot_words\n",
    "    train_df[\"ratio_eng_words\"] = train_df[\"nr_eng_words\"] / train_tot_eng_words\n",
    "\n",
    "    # Test Set: recompute features (testing time) and label (testing time), if applicable\n",
    "    test_df[\"ratio_tokens\"] = test_df[\"nr_tokens\"] / train_tot_tokens\n",
    "    test_df[\"ratio_words\"] = test_df[\"nr_words\"] / train_tot_words\n",
    "    test_df[\"ratio_eng_words\"] = test_df[\"nr_eng_words\"] / train_tot_eng_words\n",
    "    \n",
    "    # Export both\n",
    "    train_df.to_csv(join(dn, \"train.csv\"))\n",
    "    test_df.to_csv(join(dn, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Pipeline\n",
    "\n",
    "The following cell finalizes the setup of training and test data, according to the need of specific models (e.g., a certain learning algorithms expects normalized data) and apply other improvemenetss stragies like re-sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Validation Round Nr. 0###\n",
      "Going to train a Logistic Regression\n",
      "* Scaling features using StandardScaler()\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "Going to train a Stratified\n",
      "* No feature scaling\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "\n",
      "### Validation Round Nr. 1###\n",
      "Going to train a Logistic Regression\n",
      "* Scaling features using StandardScaler()\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "Going to train a Stratified\n",
      "* No feature scaling\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "\n",
      "### Validation Round Nr. 2###\n",
      "Going to train a Logistic Regression\n",
      "* Scaling features using StandardScaler()\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "Going to train a Stratified\n",
      "* No feature scaling\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "\n",
      "### Validation Round Nr. 3###\n",
      "Going to train a Logistic Regression\n",
      "* Scaling features using StandardScaler()\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "Going to train a Stratified\n",
      "* No feature scaling\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "\n",
      "### Validation Round Nr. 4###\n",
      "Going to train a Logistic Regression\n",
      "* Scaling features using StandardScaler()\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "Going to train a Stratified\n",
      "* No feature scaling\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "\n",
      "### Validation Round Nr. 5###\n",
      "Going to train a Logistic Regression\n",
      "* Scaling features using StandardScaler()\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "Going to train a Stratified\n",
      "* No feature scaling\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "\n",
      "### Validation Round Nr. 6###\n",
      "Going to train a Logistic Regression\n",
      "* Scaling features using StandardScaler()\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "Going to train a Stratified\n",
      "* No feature scaling\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "\n",
      "### Validation Round Nr. 7###\n",
      "Going to train a Logistic Regression\n",
      "* Scaling features using StandardScaler()\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "Going to train a Stratified\n",
      "* No feature scaling\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "\n",
      "### Validation Round Nr. 8###\n",
      "Going to train a Logistic Regression\n",
      "* Scaling features using StandardScaler()\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "Going to train a Stratified\n",
      "* No feature scaling\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "\n",
      "### Validation Round Nr. 9###\n",
      "Going to train a Logistic Regression\n",
      "* Scaling features using StandardScaler()\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "Going to train a Stratified\n",
      "* No feature scaling\n",
      "\tBalancing training data using SMOTE(random_state=42)\n",
      "\t\tFitting model... DONE!\n",
      "\tNo re-sampling of training data\n",
      "\t\tFitting model... DONE!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    \"cvss\",\n",
    "    \"nr_tokens\",\n",
    "    \"nr_words\",\n",
    "    \"nr_eng_words\",\n",
    "    \"ratio_tokens\",\n",
    "    \"ratio_words\",\n",
    "    \"ratio_eng_words\"\n",
    "]\n",
    "label = \"IS_OUT_OF_BOUND_WRITE\"\n",
    "\n",
    "classifiers = {\n",
    "    \"logreg\": {\n",
    "        \"name\": \"Logistic Regression\",\n",
    "        \"estimator\": LogisticRegression(random_state=42),\n",
    "        \"scaler\": StandardScaler()\n",
    "    },\n",
    "    \"stratified\": {\n",
    "        \"name\": \"Stratified\",\n",
    "        \"estimator\": DummyClassifier(strategy=\"stratified\", random_state=42),\n",
    "        \"scaler\": None\n",
    "    }\n",
    "}\n",
    "\n",
    "balancers = {\n",
    "    \"over\": {\n",
    "        \"name\": \"SMOTE\",\n",
    "        \"estimator\": SMOTE(random_state=42)\n",
    "    },\n",
    "    \"none\": {\n",
    "        \"name\": \"None\",\n",
    "        \"estimator\": None\n",
    "    }\n",
    "}\n",
    "\n",
    "splits_dir = \"../data/dataset/stratified-splits\"\n",
    "for d in sorted(listdir(splits_dir), key=int):\n",
    "    print(f\"### Validation Round Nr. {d}###\")\n",
    "    dir_path = join(splits_dir, d)\n",
    "    if not isdir(dir_path):\n",
    "        continue\n",
    "    train_df = pd.read_csv(join(dir_path, \"train.csv\"), index_col=\"cve\")\n",
    "    test_df = pd.read_csv(join(dir_path, \"test.csv\"), index_col=\"cve\")\n",
    "    \n",
    "    base_train_X = train_df[features].to_numpy()\n",
    "    base_train_y = train_df[label].to_numpy()\n",
    "    base_test_X = test_df[features].to_numpy()\n",
    "    test_y = test_df[label].to_numpy()\n",
    "\n",
    "    # Main loop\n",
    "    for clf_id, clf in classifiers.items():\n",
    "        print(f\"Going to train a {clf['name']}\")\n",
    "         # Feature Scaling: fit the scaler on training data, and transform both sets\n",
    "        if clf[\"scaler\"]:\n",
    "            print(f\"* Scaling features using {clf['scaler']}\")\n",
    "            scaler = clf[\"scaler\"].fit(base_train_X)\n",
    "            train_X, test_X = scaler.transform(base_train_X), scaler.transform(base_test_X)\n",
    "        else:\n",
    "            print(f\"* No feature scaling\")\n",
    "            train_X, test_X = base_train_X.copy(), base_test_X.copy()\n",
    "        train_y = base_train_y.copy()\n",
    "        \n",
    "        for bal_id, bal in balancers.items():\n",
    "            # Data Balancing\n",
    "            if bal[\"estimator\"]:\n",
    "                print(f\"\\tBalancing training data using {bal['estimator']}\")\n",
    "                try:\n",
    "                    work_train_X, work_train_y = bal[\"estimator\"].fit_resample(train_X, train_y)\n",
    "                except:\n",
    "                    work_train_X, work_train_y = train_X.copy(), train_y.copy()\n",
    "            else:\n",
    "                print(f\"\\tNo re-sampling of training data\")\n",
    "                work_train_X, work_train_y = train_X.copy(), train_y.copy()\n",
    "            \n",
    "            # Training time\n",
    "            print(f\"\\t\\tFitting model...\", end=\" \")\n",
    "            estimator = clf[\"estimator\"].fit(work_train_X, work_train_y)\n",
    "            train_y_pred = estimator.predict(work_train_X)\n",
    "            print(f\"DONE!\")\n",
    "            \n",
    "            # Testing time\n",
    "            test_y_pred = estimator.predict(test_X)\n",
    "            tn, fp, fn, tp = confusion_matrix(test_y, test_y_pred).ravel()\n",
    "            accuracy = accuracy_score(test_y, test_y_pred)\n",
    "            precision = precision_score(test_y, test_y_pred, zero_division = 0)\n",
    "            recall = recall_score(test_y, test_y_pred, zero_division = 0)\n",
    "            f1 = f1_score(test_y, test_y_pred, zero_division = 0)\n",
    "            den = math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "            mcc = (tn*tp - fp*fn) / den if not den == 0 else 0\n",
    "\n",
    "            # Store results\n",
    "            test_y_pred_df = pd.DataFrame(data=test_y_pred, index=test_df.index, columns=[\"test_pred\"])\n",
    "            performance = {\n",
    "                \"split\": d,\n",
    "                \"tn\": tn,\n",
    "                \"fp\": fp,\n",
    "                \"fn\": fn,\n",
    "                \"tp\": tp,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1,\n",
    "                \"mcc\": mcc\n",
    "            }\n",
    "            perf_df = pd.Series(performance).to_frame().T\n",
    "            perf_df.set_index(\"split\", inplace=True)\n",
    "            \n",
    "            model_id = f\"{clf_id}_{bal_id}\"\n",
    "            out_dir = f\"../ml_results/stratified-splits/{d}/{model_id}\"\n",
    "            if not exists(out_dir):\n",
    "                makedirs(out_dir)\n",
    "            test_y_pred_df.to_csv(join(out_dir, \"test_pred.csv\"))\n",
    "            perf_df.to_csv(join(out_dir, \"performance.csv\"))\n",
    "            \n",
    "            del work_train_X, work_train_y\n",
    "        del train_X, train_y, test_X\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
